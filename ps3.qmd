---
title: "Problem Set #3"
subtitle: "BST 258: Causal Inference -- Theory and Practice"
author: "Salvador Balkus"
date: ""
format:
  pdf:
    documentclass: scrartcl
    papersize: letter
    fontsize: 11pt
    geometry:
      - margin=1in
      - heightrounded
    number-sections: false
    colorlinks: true
    link-citations: true
    callout-appearance: simple
    callout-icon: false
    # figure options
    fig-width: 6
    fig-asp: 0.618
    fig-cap-location: bottom
    # code block options
    code-line-numbers: false
    code-block-bg: false
    highlight-style: nord
bibliography: refs.bib
---

```{r}
#| echo: false
#| message: false
#| label: global-setup
# NOTE: The immediately following line loads an renv environment located at the
#       nearest "top-level" directory, as marked by a `.here` file, which is
#       located by the here::here() function. This would be a useful tool if,
#       say, this template.qmd file was not located at the top-level directory.
#       Here, renv should activate automatically when this file is opened.
#renv::load(here::here())
library(here)
library(fMultivar)
library(tidyverse)
library(SuperLearner)
library(glmnet)
library(earth)
library(e1071)
library(boot)

```

\footnotesize

{{< pagebreak >}}

## Q1: Deriving the EIF of the Covariance

::: {.callout-note title="Answer"}
The EIF of $\Psi(P) = Cov_P(A,Y) = E_P((Y - E_P(Y))(A - E_P(A)))$ is

$$\frac{d}{dt}E_{P_t}\Big((Y - E_{P_t}(Y))(A - E_{P_t}(A))\Big)\Bigr\rvert_{t = 0}$$ Let $P_t = t\tilde{P} + (1 - t)P$ which is a parametric submodel passing through $P$, and let $\tilde{a}$ and $\tilde{y}$ denote fixed values of $A$ and $Y$ under $\tilde{P}$, respectively. Applying the chain rule hint, we have that the above is

\begin{align*}
  \frac{d}{dt}E_{P_t}E_{P_t}\Big((Y - E_{P_t}(Y))(A - E_{P_t}(A))\Big)\Bigr\rvert_{t = 0} = \\
  E_{P}\Big((-\frac{d}{dt}E_{P_t}(Y)\Bigr\rvert_{t = 0})(A - E_{P}(A))\Big) + \\
  E_{P}\Big((-\frac{d}{dt}E_{P_t}(A)\Bigr\rvert_{t = 0})(Y - E_{P}(Y))\Big) + \\
  (\tilde{y} - E_{P}(Y))(\tilde{a} - E_P(A)) - \Psi(P) =\\
  -(\tilde{y} - E_P(Y))\underbrace{E_P(A - E_P(A))}_{=0} - (\tilde{a} - E_P(A))\underbrace{E_P(Y - E_P(Y))}_{=0} + 
  (\tilde{y} - E_{P}(Y))(\tilde{a} - E_P(A)) - \Psi(P)
\end{align*}

which implies

$$\frac{d}{dt}E_{P_t}E_{P_t}\Big((Y - E_{P_t}(Y))(A - E_{P_t}(A))\Big)\Bigr\rvert_{t = 0} = (Y - E_{P}(Y))(A - E_P(A)) - \Psi(P)$$

is the EIF of the covariance, completing the proof.
:::

{{< pagebreak >}}

## Q2: Deriving the EIF of the Expected Conditional Covariance

::: {.callout-note title="Answer"}
Now, $\Psi(P) = E(Cov_P(Y, A | L)) = E_P((Y - E_P(Y | L))(A - E_P(A | L)))$. Following the same chain rule results on the covariance from Question 1, we have that

\begin{align*}
\frac{d}{dt}\Psi(P)\rvert_{t = 0} = \frac{d}{dt}E_P((Y - E_P(Y | L))(A - E_P(A | L)))\rvert_{t = 0} = \\
  E_{P}\Big((-\frac{d}{dt}E_{P_t}(Y|L)\Bigr\rvert_{t = 0})(A - E_{P}(A|L))\Big) + \\
  E_{P}\Big((-\frac{d}{dt}E_{P_t}(A|L)\Bigr\rvert_{t = 0})(Y - E_{P}(Y|L))\Big) + \\
  (\tilde{y} - E_{P}(Y|L))(\tilde{a} - E_P(A|L)) - \Psi(P)
\end{align*}

Then, applying the hint, we can plug

$$-\frac{d}{dt}E_{P_t}(Y | L)\rvert_{t = 0} = - \frac{I_{\tilde{y}}}{f(y)}\Big(\tilde{y} - E_P(Y|L)\Big)$$ 

and

$$-\frac{d}{dt}E_{P_t}(A | L)\rvert_{t = 0} = - \frac{I_{\tilde{a}}}{f(a)}\Big(\tilde{a} - E_P(A|L)\Big)$$

into the first two terms of the above to get

\begin{align*}
E_{P}\Big((-\frac{d}{dt}E_{P_t}(Y|L)\Bigr\rvert_{t = 0})(A - E_{P}(A|L))\Big) + \\
  E_{P}\Big((-\frac{d}{dt}E_{P_t}(A|L)\Bigr\rvert_{t = 0})(Y - E_{P}(Y|L))\Big)
-E_P\Big(\frac{I_{\tilde{y}}}{f(y)}\Big(\tilde{y} - E_P(Y|L)\Big) - \\
E_P(\frac{I_{\tilde{a}}}{f(a)}\Big(\tilde{a} - E_P(A|L)) \\
= -\frac{I_{\tilde{y}}}{f(y)}\Big(\tilde{y} - E_P(Y | L)\Big)\Big(\underbrace{E(A) - E(E(A|L))}_{= E(A) - E(A) = 0}\Big)\\
 -\frac{I_{\tilde{a}}}{f(a)}\Big(\tilde{a} - E_P(A | L)\Big)\Big(\underbrace{E(Y) - E(E(Y|L))}_{= E(Y) - E(Y) = 0}\Big)\\
= 0 + 0 = 0
\end{align*}

Therefore, the influence function of the conditional covariance is

$$\frac{d}{dt}\Psi(P)\rvert_{t = 0} = \Big(Y - E(Y|L)\Big)\Big(A - E(A|L)\Big) - \Psi(P)$$
:::

{{< pagebreak >}}

## Q3: The One-Step Estimator

The subparts below implement a simulation study that generates data, computes the one-step (OS) estimator, and plots results.


::: {.callout-note title="Part (a)"}

First, we define a data generating process below, which simulates $A$ and $Y$ as bivariate normal random variables with covariance dependent on L and conditonal mean a linear function of $L$.

```{r, cache=T}
# Define necessary functions to generate the data
cov_func <- function(L){(L / 10) + 0.1}

dgp <- function(n){
  # Generate the data
  m = 5
  p = 0.4
  lambda = 10
  L1 = rbinom(n, m, p)
  L2 = rpois(n, lambda)
  AY = sapply(cov_func(L1), function(x){rnorm2d(1, x)})
  A = AY[1,] + L1 + 0.1 * L2
  Y = AY[2,] + 0.5 * L1 + 0.1 * L2
  dat = data.frame(L1 = L1, L2 = L2, A = A, Y = Y)
  
  # Get the true value of the parameter
  x = unique(L1)
  psi = sum(cov_func(x) * dbinom(x, m, p))
  
  return(list(dat = dat, psi = psi))
}
```

Then, we define a function to compute the OS estimator of the conditional covariance, followed by a function to simulate DGP draws, estimate nuisance parameters using a (correctly specified) linear regression model, and compute OS estimates across many repetitions.

```{r, cache=T}
# One-step estimator for expected conditional covariance
onestep.condcov <- function(A, Y, EA, EY){
  IF = (A - EA) * (Y - EY)
  return(mean(IF))
}

# Function to simulate data, fit correct parametric nuisances, and compute OS
sim.onestep <- function(n){
  sim = dgp(n)
  dat = sim$dat

  # Fit nuisance estimators
  EA.model = lm(A ~ L1 + L2, dat)
  EA = predict(EA.model, dat)
  EY.model = lm(Y ~ L1 + L2, dat)
  EY = predict(EY.model, dat)

  # Compute estimator and CIs
  res = onestep.condcov(dat$A, dat$Y, EA, EY)

  # Return the bias
  return(res - sim$psi)
}

# Function to run many replicates of a simulation and summarize results
simulate <- function(n, iters){
  replicate(iters, sim.onestep(n))
}
```

Below, we run the simulation across a range of sample sizes, computing the bias of the estimator with *correctly-specified parametric nuisance estimates*.

```{r, cache=T}
# Run simulation
n = c(100, 400, 900, 1600, 2500)
iters = 100
sim.result = sapply(n, function(N){simulate(N, iters)})

# Get simulation results into a dataframe
sim.df = data.frame(sim.result) 
colnames(sim.df) = as.character(n)
sim.df = sim.df %>% pivot_longer(as.character(n))
colnames(sim.df) = c("Samples", "Bias")
sim.df$Samples = as.numeric(sim.df$Samples)

```

The plot below displays the sampling distribution of the empirical bias of the OS estimate in the given bivariate normal simulation across sample sizes. From this, we can see that the estimator progressively converges to a normal distribution, with tighter standard error at higher sample sizes. Hence, this provides empirical evidence that the OS estimator with correctly-specific parametric nuisance estimators is asymptotically normal.

```{r, cache=T}
ggplot(sim.df) + 
  geom_histogram(aes(x = Bias), 
                 fill = "gray", bins = 40) + 
  facet_grid(rows = "Samples", ) + 
  geom_vline(aes(xintercept = 0)) + 
  theme_light() +
  ggtitle("One-Step Sampling Bias (Parametric Nuisances)")
```

Summary statistics from the plot above are computed and displayed below, including the mean bias, empirical variance, and 95% confidence intervals based on the normal approximation. This table is plotted as a line plot with error bars below. We can see that the mean bias does appear to generally decrease with sample sizes, showing that the OS estimator converges to the truth. The variance also decreases with sample size as well.

```{r, cache=T}
summary.stats = sim.df %>% group_by(Samples) %>% 
  summarize(`Mean Bias` = mean(Bias), `Emp. Var.`= var(Bias),
            `Upper 95%` = mean(Bias) + 1.96 * sqrt(var(Bias)), 
            `Lower 95%` = mean(Bias) - 1.96 * sqrt(var(Bias)))
summary.stats
```

```{r, cache=T}
ggplot(summary.stats) + 
  geom_line(aes(x = Samples, y = `Mean Bias`), 
            linewidth = 1.1, color = "blue") + 
  geom_errorbar(aes(x = n, ymin = `Lower 95%`, ymax = `Upper 95%`), 
                linewidth = 1.1, color = "blue", width = 100) + 
  geom_hline(aes(yintercept = 0)) + 
  theme_light() + 
  ggtitle("One-Step Estimator, Parametric Nuisance Estimators - Bias")
```

Overall, the OS estimator appears to perform as expected when *correctly-specified parametric nuisance models* are used.

:::

::: {.callout-note title="Part (b)"}

Next, we repeat the same simulation from (a), but using super learning to estimate nuisance parameters instead of a correctly-specified model. To observe results when the correct parametric model is not included, but more flexible models are used, the super learner includes three learners: a decision tree, a GAM, and a random forest. These are reasonably flexible models that can be fit in a computationally-efficient enough manner for simulation. The code to simulate super-learned estimates and compute the OS estimates (and their bias) is below.

```{r, cache=T}

# Function to simulate OS estimator with super-learned nuisances
sim.onestep.sl <- function(n){
  sim = dgp(n)
  dat = sim$dat
  
  # Fit nuisance estimators via SL
  lib = c("SL.rpart", "SL.gam", "SL.ranger")
  X = dat[,c("L1","L2")]
  EA.sl = SuperLearner(Y = dat$A, X = X, SL.library = lib)
  EA = predict(EA.sl, X)$pred
  EY.sl = SuperLearner(Y = dat$Y, X = X, SL.library = lib)
  EY = predict(EY.sl, X)$pred

  # Compute OS estimates and CIs
  res = onestep.condcov(dat$A, dat$Y, EA, EY)
  return(res - sim$psi)
}

# Function to run many simulations with super-learned nuisances
simulate.sl <- function(n, iters){
  replicate(iters, sim.onestep.sl(n))
}

```

Below, we run the simulation across a range of sample sizes, computing the bias of the estimator with *flexible super learning nuisance estimatators*.

```{r, cache=T}
# Run simulation
iters = 100
sim.result.sl = sapply(n, function(N){simulate(N, iters)})

# Get simulation results into a dataframe
sim.df.sl = data.frame(sim.result.sl) 
colnames(sim.df.sl) = as.character(n)
sim.df.sl = sim.df.sl %>% pivot_longer(as.character(n))
colnames(sim.df.sl) = c("Samples", "Bias")
sim.df.sl$Samples = as.numeric(sim.df.sl$Samples)

```
The plot below again displays the sampling distribution of the empirical bias of the OS estimate in the given bivariate normal simulation across sample sizes, this time using flexible super-learned estimators for nuisance parameters. We can see that this estimator also progressively converges to a normal distribution, with tighter standard error at higher sample sizes. Hence, the OS estimator appears to be asymptotically normal even when nuisances are estimated via flexible regression techniques.

```{r, cache=T}
ggplot(sim.df.sl) + 
  geom_histogram(aes(x = Bias), 
                 fill = "gray", bins = 40) + 
  facet_grid(rows = "Samples", ) + 
  geom_vline(aes(xintercept = 0)) + 
  theme_light() +
  ggtitle("One-Step Sampling Bias (Flexible Nuisances)")
```

Summary statistics from the plot above are computed and displayed below, including the mean bias, empirical variance, and 95% confidence intervals based on the normal approximation. The table is plotted as a line plot with error bars below. Again, we observe that the mean bias does appear to generally decrease with sample sizes, showing that the OS estimator converges to the truth. The variance also decreases with sample size as well.

```{r, cache=T}
summary.stats.sl = sim.df.sl %>% group_by(Samples) %>% 
  summarize(`Mean Bias` = mean(Bias), `Emp. Var.`= var(Bias),
            `Upper 95%` = mean(Bias) + 1.96 * sqrt(var(Bias)), 
            `Lower 95%` = mean(Bias) - 1.96 * sqrt(var(Bias)))
summary.stats.sl
```

```{r, cache=T}
ggplot(summary.stats.sl) + 
  geom_line(aes(x = Samples, y = `Mean Bias`), 
            linewidth = 1.1, color = "orange") + 
  geom_errorbar(aes(x = n, ymin = `Lower 95%`, ymax = `Upper 95%`), 
                linewidth = 1.1, color = "orange", width = 100) + 
  geom_hline(aes(yintercept = 0)) + 
  theme_light() + 
  ggtitle("One-Step Estimator, Flexible Nuisance Estimators - Bias")

```

:::

::: {.callout-note title="Part (c)"}

We can see that there was little difference in the performance of the one-step estimator for this simple data generating process, no matter whether we used correctly-specified parametric estimator or flexible, super-learned regression estimators for nuisance estimation. Both were asymptotically normal, with similar bias and variance, and large sample sizes yielding better variance.

Since we never truly know whether an underlying parametric model is correct, we often prefer more flexible machine-learning-based models, and with the one-step estimator, we can generally be assured that (in simple settings) these flexible models will not drastically change our results.

:::

{{< pagebreak >}}

## Q4: Variance-Weighted Treatment Effect

::: {.callout-note title="Derivation of VTE Equation"}

The variance-weighted treatment effect is

$$\psi^{VTE} = \Psi(P) = \frac{E(Cov(Y,A|L))}{E(Var(A|L))}$$
In this part, our goal is to show

$$\Psi(P) = \frac{E(Cov(Y,A|L))}{E(Var(A|L))} = E(w(L)E(Y^1 - Y^0|L))$$
where $w(L) = \frac{Var(A|L)}{E(Var(A|L))}$. To do this, we first note that by the definition of conditional variance, since $Y^a$ is independent of $A$, we have

\begin{align*}
E(w(L)E(Y^1 - Y^0|L)) = \frac{E(Var(A|L)E(Y^1 - Y^0|L))}{E(Var(A|L))} \\
=  \frac{E(E((A-E(A|L))^2|L)E(Y^1 - Y^0|L))}{E(Var(A|L))}\\
= \frac{E(E((A-E(A|L))^2(Y^1 - Y^0)|L))}{E(Var(A|L))}\\
= \frac{E(E((A-E(A|L))(AY^1 - AY^0 - E(AY^1 - AY^0|L))|L))}{E(Var(A|L))}
\end{align*}

Since it is known that $Y = AY^1 + (1 - A)Y^0$, we note this implies $AY^1 - AY^0 = Y - Y^0$, which we can plug into the above expression. Furthermore, since $Y^0$ is a constant, we can move it outside the expectation (by linearity of expectation) and so

\begin{align*}
\frac{E(E((A-E(A|L))(AY^1 - AY^0 - E(AY^1 - AY^0|L))|L))}{E(Var(A|L))} \\
= \frac{E(E((A-E(A|L))(Y - Y^0 - E(Y - Y^0|L))|L))}{E(Var(A|L))} \\
= \frac{E(E((A-E(A|L))(Y - Y^0 + Y^0 - E(Y|L))|L))}{E(Var(A|L))} \\
= \frac{E(E((A-E(A|L))(Y - E(Y|L))|L))}{E(Var(A|L))} \\
= \frac{E(Cov(A,Y|L))}{E(Var(A|L))}
\end{align*}

proving the desired expression holds.

:::

{{< pagebreak >}}


::: {.callout-note title="Derivation of Influence Function of VTE"}

To compute the EIF, we simply apply the quotient rule to the fraction and plug in the EIF of the conditional covariance computed previously (noting that $E(Var(A|L)) = E(Cov(A, A|L))$, for which we can plug in the EIF of the conditional covariance of $A$ with itself):

\begin{align*}
\frac{d}{dt}\Psi(P_t)\Bigr\rvert_{t = 0} = \frac{d}{dt}\frac{E_{P_t}(Cov_{P_t}(Y,A|L))}{E_{P_t}(Var_{P_t}(A|L))} \\
= \frac{\frac{d}{dt}E_{P_t}(Cov_{P_t}(Y,A|L))\Bigr\rvert_{t = 0} \cdot E_{P}(Cov_{P}(A,A|L))}{E_P(Cov_P(A, A|L))^2} - \frac{\frac{d}{dt}E_{P_t}(Cov_{P_t}(A,A|L))\Bigr\rvert_{t = 0} \cdot E_{P}(Cov_{P}(Y,A|L))}{E_P(Cov_P(A, A|L))^2} \\
= \frac{(Y - E_P(Y|L))(A - E_P(A|L))}{E_P(Cov_P(A, A|L))} - \frac{(A - E_P(A | L))^2E_P(Cov_P(Y,A|L))}{E_P(Cov_P(A, A|L))^2}\\ -  \frac{E_P(Cov_P(Y,A|L)}{E_P(Cov_P(Y,A|L)} + \frac{E_P(Cov_P(Y,A|L))}{E_P(Cov_P(Y,A|L))}\\
= \frac{(A - E_P(A |L))^2}{E(Var(A|L))}\Big(\frac{Y - E(Y|L)}{A - E(A|L)} - \Psi(P)\Big)
\end{align*}

Therefore, the EIF of the VTE is

$$\frac{(A - E_P(A |L))^2}{E(Var(A|L))}\Big(\frac{Y - E(Y|L)}{A - E(A|L)} - \Psi(P)\Big)$$

:::

::: {.callout-note title="Derivation of One-Step Estimator of VTE"}

We can obtain the one-step estimator of the VTE by plugging in the EIF $\phi$ into the general one-step estimator form

$$\Psi(\hat{P}_n) + \frac{1}{n}\sum_{i=1}^n\phi(O_i, \hat{P}_n)$$
Using the EIF result from the previous part of this problem at $P = \hat{P}_n$, we have that

\begin{align*}
\Psi(\hat{P}_n) + \frac{1}{n}\sum_{i=1}^n\phi(O_i, \hat{P}_n) \\
= \Psi(\hat{P}_n) + \frac{1}{n}\sum_{i=1}^n \frac{(A_i - E_{\hat{P}_n}(A_i |L))^2}{E_{\hat{P}_n}(Var_{\hat{P}_n}(A_i|L_i))}\Big(\frac{Y_i - E_{\hat{P}_n}(Y_i|L_i)}{A_i - E_{\hat{P}_n}(A_i|L_i)} - \Psi(\hat{P}_n)\Big)\\
= \Psi(\hat{P}_n) + \frac{1}{n}\sum_{i=1}^n \frac{(A_i - E_{\hat{P}_n}(A_i |L))(Y_i - E_{\hat{P}_n}(Y_i|L_i))}{E_{\hat{P}_n}(Var_{\hat{P}_n}(A_i|L_i))} - \frac{(A_i - E_{\hat{P}_n}(A_i |L))^2}{E_{\hat{P}_n}(Var_{\hat{P}_n}(A_i|L_i))}\Psi(\hat{P}_n)\\
\end{align*}

But note that if $\Psi(\hat{P}_n) \rightarrow \Psi(\tilde{P})$ then $\frac{1}{n}\sum_{i=1}^n (A_i - E_{\hat{P}_n}(A_i |L))^2 \cdot \frac{1}{E_{\hat{P}_n}(Var_{\hat{P}_n}(A_i|L_i))}\cdot\Psi(\hat{P}_n) \overset{P}{\rightarrow} \Psi(\tilde{P})$ by the weak law of large numbers and the continuous mapping theorem, which implies that

$$\Psi(\hat{P}_n) - \frac{1}{n}\sum_{i=1}^n \frac{(A_i - E_{\hat{P}_n}(A_i |L))^2}{E_{\hat{P}_n}(Var_{\hat{P}_n}(A_i|L_i))}\Psi(\hat{P}_n) = o_P(1)$$

so these terms cancel, and the one-step bias-corrected estimator is 

$$\frac{1}{n}\sum_{i=1}^n \frac{(A_i - E_{\hat{P}_n}(A_i |L))(Y_i - E_{\hat{P}_n}(Y_i|L_i))}{E_{\hat{P}_n}((A_i - E_{\hat{P}_n}(A_i |L))^2)} + o_P(1)$$


:::


{{< pagebreak >}}

## Q5: Variance-Weighted Treatment Effect in the NHEFS Dataset

::: {.callout-note title="Calculate the VTE between Smoking Cessation and Weight Gain"}

In this problem, we estimate the VTE in the NHEFS Dataset, previously used for Problem Set 2. First, we implement the One-Step VTE estimator using the influence function derived above. This code returns the estimate and its variance based on the influence function.

```{r, cache=T}
# Define the one-step estimator for the variance-weighted treatment effect
vte_onestep <- function(x){
  # Get intermediate variables
  A = x[,1]
  Y = x[,2]
  EA = x[,3]
  EY = x[,4]
  Acenter = A - EA
  Ycenter = Y - EY
  EVar = mean(Acenter^2)
  
  # Compute one-step estimator
  psi = mean(Acenter * Ycenter) / EVar
  
  # include analytic variance calculation for ease of use
  sigma2 = var((Acenter^2 / EVar) * ((Ycenter/Acenter) - psi))
  
  return(c(vte = psi, se = sqrt(sigma2 / length(A))))
}

```

Next, we load and clean the data, and fit nuisance estimators. Super learning with sample-splitting/cross-fitting is used to estimate the conditional means involved in the VTE influence function. Since this data has a mix of several continuous and factor variables, and $A$ is binary while $Y$ is continuous, a random forest and a multivariate adaptive regression spline are fit in the super learner ensemble. These learners were chosen because they can flexibly model potential nonlinearities and interaction terms while handling multiple types of data among the target and predictor variables. A GLM is also included as a baseline/benchmark simple model. The results of the VTE using these crossfit nuisance parameters are output below. We can see the VTE is about 3.4.

```{r, cache=T}
# Prepare the data
df = read.csv("nhefs.csv") %>% 
  select(wt82_71, qsmk, sex, age, race, education, 
         smokeintensity, smokeyrs, active, exercise, wt71) %>% 
  transform(education = factor(education), active = factor(active), 
            exercise = factor(exercise), race = factor(race)) %>%
  drop_na(wt82_71)

# Estimate nuisance parameters
lib = c("SL.glm", "SL.ranger", "SL.earth")
L = df %>% select(-wt82_71, -qsmk)
Amodel = CV.SuperLearner(Y = df$qsmk, X = L, SL.library = lib)
Ymodel = CV.SuperLearner(Y = df$wt82_71, X = L, SL.library = lib)

# Package components of one-step estimation
x = data.frame(A = df$qsmk, Y = df$wt82_71, 
               EA = Amodel$discreteSL.predict, 
               EY = Ymodel$discreteSL.predict)

# Get one-step estimate
result = vte_onestep(x)
result
```
:::



::: {.callout-note title="Analytically Calculate Standard Error and Compare to Bootstrap"}

For simplicity, the OS standard error based on the influence function was included in the OS estimator code above (for code reuse). See the first code block for Q5. The code below bootstraps the VTE estimate above using the classical nonparametric bootstrap (implemented in the `boot` package) and computes the bootstrap standard error. Both estimates are output below:

```{r, cache=T}
# Bootstrap the VTE One-step estimator statistic
vte_onestep_boot <- function(x, d){vte_onestep(x[d,])}
boot_result = boot(x, vte_onestep_boot, R = 1000)
se_boot = sqrt(var(boot_result$t[,1]))

c(bootstrapped.se = se_boot, analytic = result["se"])

```

We can see that the analytic and bootstrapped standard errors are very similar, both about 0.47. Hence, we can be confident that the influence-function-based analytic variance estimator probably performs well for this dataset.

:::

